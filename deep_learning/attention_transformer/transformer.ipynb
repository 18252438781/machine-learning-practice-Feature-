{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 项目介绍\n",
    "====\n",
    "\n",
    "我们将训练一个 **Transformer** 模型 用于将葡萄牙语翻译成英语。在此之前，建议先了解有关[文本生成](https://www.tensorflow.org/tutorials/text/text_generation?hl=zh-cn)和[注意力机制](https://www.tensorflow.org/tutorials/text/nmt_with_attention?hl=zh-cn)的相关内容。\n",
    "\n",
    "**Transformer** 模型的核心思想是**自注意力机制**（self-attention）——能注意输入序列的不同位置以计算该序列的表示的能力。**Transformer** 创建了多层**自注意力层**（self-attetion layers）组成的堆栈，下文的按比缩放的点积注意力（Scaled dot product attention）和多头注意力（Multi-head attention）部分对此进行了说明。\n",
    "\n",
    "Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。 作者采用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：\n",
    "\n",
    "时间片 tt 的计算依赖 t-1t−1 时刻的计算结果，这样限制了模型的并行能力\n",
    "\n",
    "顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象，LSTM依旧无能为力。\n",
    "\n",
    "Transformer的提出解决了上面两个问题：\n",
    "\n",
    "首先它使用了Attention机制，将序列中的任意两个位置之间的距离缩小为一个常量；\n",
    "\n",
    "其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。\n",
    "\n",
    "\n",
    "该架构的缺点是：\n",
    "\n",
    "*   对于时间序列，一个单位时间的输出是从整个历史记录计算的，而非仅从输入和当前的隐含状态计算得到。这可能效率较低。\n",
    "*   如果输入确实有时间 / 空间的关系，像文本，则必须加入一些位置编码，否则模型将有效地看到一堆单词。\n",
    "\n",
    "训练完模型后，您将能输入葡萄牙语句子，得到其英文翻译。\n",
    "\n",
    "![](https://tensorflow.google.cn/images/tutorials/transformer/attention_map_portuguese.png?hl=zh-cn)\n",
    "\n",
    "\n",
    "\n",
    "Transformer模型在论文[《Attention Is All You Need》](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)中被提出，利用Attention，去掉RNN。这篇论文的题目想说的是：你只需要用到Attention机制，完全不需要再用到RNN，就能解决很多问题。Transformer没有用到RNN，而且它的效果很好。\n",
    "\n",
    "Transformer模型总体的样子如下图所示：总体来说，还是和Encoder-Decoder模型有些相似，左边是Encoder部分，右边是Decoder部分。\n",
    "![](https://luweikxy.github.io/machine-learning-notes/natural-language-processing/self-attention-and-transformer/attention-is-all-you-need/pic/transformer-model-architecture.jpg)\n",
    "\n",
    "Encoder：输入是单词的Embedding，再加上位置编码，然后进入一个统一的结构，这个结构可以循环很多次（N次），也就是说有很多层（N层）。每一层又可以分成Attention层和全连接层，在每一层中，再额外加了一些处理，比如Skip Connection，做跳跃连接，然后还加了Normalization层。其实它本身的模型还是很简单的。\n",
    "\n",
    "Decoder：第一次输入是前缀信息，之后的就是上一次产出的Embedding，加入位置编码，然后进入一个可以重复很多次的模块。该模块可以分成三块来看，第一块也是Attention层，第二块是cross Attention，不是Self-Attention，第三块是全连接层。也用了跳跃连接和Normalization。\n",
    "\n",
    "输出：最后的输出要通过Linear层（全连接层），再通过softmax做预测。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入库、数据集、数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\xukaihui\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 TFDS 来导入 葡萄牙语-英语翻译数据集，该数据集来自于 TED 演讲开放翻译项目.\n",
    "\n",
    "该数据集包含来约 50000 条训练样本，1100 条验证样本，以及 2000 条测试样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "train_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时得到的 **train_examples** 和 **val_examples** 的类型都是 **dataset**，所以我们可以用它的 **take** 属性打印其中一个样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "mas e se estes fatores fossem ativos ?\n",
      "mas eles não tinham a curiosidade de me testar .\n",
      "\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "  for pt in pt_examples.numpy():\n",
    "    print(pt.decode('utf-8'))\n",
    "\n",
    "  print()\n",
    "\n",
    "  for en in en_examples.numpy():\n",
    "    print(en.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您不能直接在文本上训练模型。文本需要首先转换为一些数字表示。通常，您将文本转换为 ID 序列，用作嵌入的索引。\n",
    "\n",
    "Subword tokenizer 教程中演示了一种流行的实现，它构建text.BertTokenizer了为此数据集优化的子词 tokenizer ( ) 并将它们导出到saved_model中。\n",
    "\n",
    "下载并解压缩并导入saved_model："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\ted_hrlr_translate_pt_en_converter.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"ted_hrlr_translate_pt_en_converter\"\n",
    "tf.keras.utils.get_file(\n",
    "    f\"{model_name}.zip\",\n",
    "    f\"https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip\",\n",
    "    cache_dir='.', cache_subdir='', extract=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包含两个文本标记器tf.saved_model，一个用于英语，一个用于葡萄牙语。两者都有相同的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.saved_model.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['detokenize',\n",
       " 'get_reserved_tokens',\n",
       " 'get_vocab_path',\n",
       " 'get_vocab_size',\n",
       " 'lookup',\n",
       " 'tokenize',\n",
       " 'tokenizer',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该tokenize方法将一批字符串转换为一批填充的令牌 ID。此方法在标记化之前拆分标点符号、小写字母和 unicode 规范化输入。该标准化在这里不可见，因为输入数据已经标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "for en in en_examples.numpy():\n",
    "  print(en.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n",
      "[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n",
      "[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizers.en.tokenize(en_examples)\n",
    "\n",
    "for row in encoded.to_list():\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该detokenize方法尝试将这些 ID 转换回人类可读的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n ' t test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "round_trip = tokenizers.en.detokenize(encoded)\n",
    "for line in round_trip.numpy():\n",
    "  print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "较低级别的lookup方法将 token ID 转换为 token text："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability', b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage', b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip', b'##ity', b'.', b'[END]'], [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?', b'[END]'], [b'[START]', b'but', b'they', b'did', b'n', b\"'\", b't', b'test', b'for', b'curiosity', b'.', b'[END]']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizers.en.lookup(encoded)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，您可以看到分词器的“子词”方面。单词“searchability”分解为“search ##ability”，单词“serendipity”分解为“s ##ere ##nd ##ip ##ity”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对原始文本进行编码\n",
    "def tokenize_pairs(pt, en):\n",
    "    pt = tokenizers.pt.tokenize(pt)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    pt = pt.to_tensor()\n",
    "\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    # Convert from ragged to dense, padding with zeros.\n",
    "    en = en.to_tensor()\n",
    "    return pt, en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function tokenize_pairs at 0x0000025BC977CB70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function tokenize_pairs at 0x0000025BC977CB70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function tokenize_pairs at 0x0000025BC977CB70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .cache()\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "      .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置编码\n",
    "\n",
    "因为该模型并不包括任何的循环神经网络，所以此模型中不包括任何的词序信息，而这些信息是非常重要的。\n",
    "\n",
    "比如一个单词在句子中的位置或排列顺序不同，可能整个句子的意思就发生了偏差。\n",
    "\n",
    "```\n",
    "I do not like apple, but I do like banana.\n",
    "I do like apple, but I do not like banana.\n",
    "\n",
    "```\n",
    "\n",
    "上面两句话所使用的的单词完全一样，但是所表达的句意却截然相反。那么，我们需要引入词序信息来区别这两句话的意思。\n",
    "\n",
    "所以模型添加了位置编码，为模型提供一些关于单词在句子中相对位置的信息。\n",
    "\n",
    "**Transformer** 模型本身不具备像循环神经网络那样的学习词序信息的能力，所以我们需要主动地将词序信息输入模型。那么，模型原先的输入是不含词序信息的词向量，位置编码需要将词序信息和词向量结合起来形成一种新的表示输入给模型（在编码器和解码器中使用），这样模型就具备了学习词序信息的能力。\n",
    "\n",
    "计算位置编码的公式如下：  \n",
    "![](https://img-blog.csdnimg.cn/20200414105057638.png#pic_center)  \n",
    "\n",
    "\n",
    "其中， pos 是单词的位置索引，设句子长度为 L ，那么 $pos=0,1,...,L−1$。 i 是向量的某一维度，假设词向量维度 $d_{model}=512$ ，那么 $i=0,1,...,255$。\n",
    "\n",
    "举例来说，假设 $d_{model}=5$，那么在一个样本中：  \n",
    "第一个单词的位置编码为：  \n",
    "\n",
    "$$\\begin{bmatrix} sin(\\frac{0}{10000^{\\frac{2\\times 0}{5}}}) & cos(\\frac{0}{10000^{\\frac{2\\times 0}{5}}}) & sin(\\frac{0}{10000^{\\frac{2\\times 1}{5}}}) & cos(\\frac{0}{10000^{\\frac{2\\times 1}{5}}}) & sin(\\frac{0}{10000^{\\frac{2\\times 2}{5}}}) \\\\ \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "第二个单词的位置编码为：\n",
    "\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix} sin(\\frac{1}{10000^{\\frac{2\\times 0}{5}}}) & cos(\\frac{1}{10000^{\\frac{2\\times 0}{5}}}) & sin(\\frac{1}{10000^{\\frac{2\\times 1}{5}}}) & cos(\\frac{1}{10000^{\\frac{2\\times 1}{5}}}) & sin(\\frac{1}{10000^{\\frac{2\\times 2}{5}}}) \\\\ \\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    # np.arange(position)[:, np.newaxis] 增加维度  position * 1，   1 * d_model\n",
    "    # 计算后可以得到  position * d_model 维度的矩阵\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    \n",
    "    # 将 sin 应用于数组中的偶数索引（indices）；2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 将 cos 应用于数组中的奇数索引；2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # 1 * position * d_model\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, d = 50, 512\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, d))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 遮挡（Masking）\n",
    "\n",
    "$\\begin{cases}1.padding \\ mask \\\\2.lookahead mask（翻译任务中预测文本时 (decoder 部分))\\end{cases}$\n",
    "\n",
    "### 填充遮挡 （padding mask）\n",
    "\n",
    "遮挡一批序列中所有的填充标记（即将文本转换到数字向量后标记为零的位置）。这确保了模型不会将填充作为输入。在填充值 0 出现的位置 mask 输出 1，否则输出 0。\n",
    "\n",
    "当我们在确定 $Max \\ length$ 时候，对于不够长的句子肯定要做 $padding$ 但是对于为 0 的那一部分在 $softmax$ 时候会变为 1：  \n",
    "        回顾 $softmax$ 函数:\n",
    "$$\n",
    "\\sigma (\\mathbf {z} )_{i}= \\frac {e^{z_i} } {\\sum _{j=1} ^ {K} e^ {z_j} }  \n",
    "$$\n",
    "\n",
    "$$ $$\n",
    "\n",
    "$e^0$ 是 1, 是有值的, 这样的话 $softmax$ 中被 $padding$ 的部分就参与了运算, 就等于是让无效的部分参与了运算, 这样肯定不对, 这时就需要做一个 $mask$ 让这些无效区域不参与运算, 我们一般给无效区域加一个很大的负数的偏置, 也就是:\n",
    "\n",
    "$$z_{illegal}=z_{illegal}+bias_{illegal}$$  \n",
    "$$bias_{illegal}\\to-\\infty$$  \n",
    "$$e^{z_{illegal}}\\to0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # 添加额外的维度来填充\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lookahead mask\n",
    "\n",
    "前瞻遮挡用于遮挡未来的信息，在训练和预测阶段使用。这意味着，如果要预测第三个词，将仅使用第一个和第二个词。与此类似，预测第四个词，仅使用第一个，第二个和第三个词，依此类推。 举例来说：  \n",
    "![](https://img-blog.csdnimg.cn/20200414110922673.png#pic_center)  \n",
    "比如说输入是一句话 “I have a dream” 总共 4 个单词， 这里就会形成一张 4x4 的注意力机制（在下面介绍）的图。\n",
    "\n",
    "I 作为第一个单词，只能有和 I 自己的 attention。have 作为第二个单词，有和 I, have 两个 attention。 a 作为第三个单词，有和 I, have, a 前面三个单词的 attention。到了最后一个单词 dream 的时候，才有对整个句子 4 个单词的 attention。\n",
    "\n",
    "\n",
    "这里的 **tf.linalg.band_part(input, num_lower, num_upper)** 函数可以返回一个三角矩阵，**input** 是输入的矩阵；**num_lower** 是下三角中保留的行数；**num_upper** 是上三角中保留的行数，当 **num_lower** 或 **num_upper** 等于 -1 时，表示下三角或上三角全部保留。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.7363788 0.1665951 0.8911698]], shape=(1, 3), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "print(x)\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muti-head attention\n",
    "\n",
    "在前面的基础上，我们已经有了词向量矩阵和位置嵌入了，例如有一些样本。维度是：$[batch size, \\ sequence \\ length]$, 再在字典中找到对应的字向量，变为：$[batch size, \\ sequence \\ length, \\ embedding \\ dimension]$, 同时我们再加上位置嵌入（位置嵌入维度一致，直接元素相加即可），相加后的维度还是 $[batch size, \\ sequence \\ length, \\ embedding \\ dimension]$  \n",
    "要了解 Muti-Head Attention，首先要知道 self-attention,Multi 无非是在其基础上并行了多个头而已。\n",
    "\n",
    "#### self Attention\n",
    "\n",
    "Attention 机制的创新点就在于这里，为了学到多重含义的表示，我们想让一个字的向量包含这句话所有字的一个相关程度，那么首先初始化三个权重矩阵 $W_Q、W_K、W_V$，然后将 $X_{embedding}$ 与这三个权重矩阵相乘，得到 $Q、K、V$  \n",
    "也就是：  \n",
    "$$ Q=X_{embedding} W_K  $$  \n",
    "$$W_Q \\ K=X_{embedding} $$\n",
    "$$V=X_{embedding} W_V$$\n",
    "\n",
    "下面用图来理解更舒适！  \n",
    "[![self attension](https://luweikxy.github.io/machine-learning-notes/natural-language-processing/self-attention-and-transformer/attention-is-all-you-need/pic/self-attention.jpg)\n",
    "\n",
    "得到了 $Q、K、V$ 之后 那么我们用 $q\\times k$ 也就是对于一个字（中文是字，英文是词）它的 score 包含所有的自身 $q$ 和别的字的 $k$ 相乘, 当然这里相乘肯定是和 $k$ 的转置相乘哈！从而就可以得到一个注意力矩阵！(点积：两个向量越相似，点积则越大！) 这里你会观察到，对角线上也就是每个字，自己对自己的相关程度，一行就是一个字中所有字与它的相关性。然后再对每一行做归一化（$softmax$），这样就保证对一个字来说，所有字与它的相关程度概率和为 1！  \n",
    "[![](https://img-blog.csdnimg.cn/20200411153038943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center%20=300x200)](https://img-blog.csdnimg.cn/20200411153038943.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center%20=300x200)  \n",
    "然后论文中又除了一个 $\\sqrt{d_k}$, 是为了把注意力矩阵变成标准正态分布，使得 softmax 归一化后的结果更加稳定，以便于反向传播时候获取平衡的梯度，最后将注意力矩阵给 $V$ 加权，为啥要给 $V$ 加权，其实就是因为注意力矩阵维度是 $[batch \\ size, \\ sequence \\ length, \\ sequence \\ length]$，而 $V$ 维度是 $[batch \\ size ,\\ sequence \\ length, \\ embedding \\ dimension]$ 为了使得维度保持不变，则乘以 $V$ 后为: $[batch \\ size ,\\ sequence \\ length, \\ embedding \\ dimension]$, 从而再次和 $X_{embedding}$ 的维度相同了，是不是很妙！  \n",
    " \n",
    "假设 Q 和 K 的均值为 0，方差为 1。它们的矩阵乘积将有均值为 0，方差为 $d_k$​。因此， $d_k$​的平方根被用于缩放，因为，Q 和 K 的矩阵乘积的均值本应该为 0，方差本应该为 1，这样会获得一个更平缓的 **softmax**。\n",
    "\n",
    "\n",
    "### Scaled dot-product attention\n",
    "\n",
    "**Scaled dot-product attention** 的结构为：  \n",
    "![](https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png)\n",
    "\n",
    "**Transformer** 使用的注意力函数有三个输入：Q（请求（query））、K（主键（key））、V（数值（value））。用于计算注意力权重的等式为：  \n",
    "$${Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$ \n",
    "\n",
    "这里的 $d_k$​ 其实就是词嵌入向量的维度 $d_{model}$。\n",
    "\n",
    "\n",
    "\n",
    "TODO 需要进一步解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度。\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
    "    虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
    "    但是 mask 必须能进行广播转换以便求和。\n",
    "\n",
    "    参数:\n",
    "    q: 请求的形状 == (..., seq_len_q, depth)\n",
    "    k: 主键的形状 == (..., seq_len_k, depth)\n",
    "    v: 数值的形状 == (..., seq_len_v, depth_v)\n",
    "    mask: Float 张量，其形状能转换成\n",
    "          (..., seq_len_q, seq_len_k)。默认为None。\n",
    "\n",
    "    返回值:\n",
    "    输出，注意力权重\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # 缩放 matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # 将 mask 加入到缩放的张量上。\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化，因此分数相加等于1,\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里我们将遮挡（mask）与 -1e9（接近于负无穷）相乘，其目标是将这些单元在 **softmax** 中归零，因为 **softmax** 的较大负数输入在输出中接近于零。\n",
    "\n",
    "当 **softmax** 在 K 上进行归一化后，它的值决定了将 K 对应的 V 分配到 Q 的重要程度。\n",
    "\n",
    "输出表示注意力权重和 V（数值）向量的乘积。这使得要关注的词保持原样，而无关的词将被清除掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出注意力权重\n",
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print('Attention weights are:')\n",
    "  print(temp_attn)\n",
    "  print('Output is:')\n",
    "  print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-head attention\n",
    "\n",
    "**Multi-head attention** 就是把上面的 **Scaled dot-product attention** 操作执行 H 次，然后把输出合并，如下图所示：  \n",
    "![](https://img-blog.csdnimg.cn/20200414115306674.png#pic_center)  \n",
    "举例来说，如果 H=8，那么我们要将 V，K，Q 各分成 8 份，对每一份进行 **Scaled dot-product attention** 操作，最后将得到的结果合并起来。\n",
    "\n",
    "那这样的好处是什么呢？可以让Attention有更丰富的层次。可以分别从多个不同角度来看待Attention。这样的话，输入x，对于不同的 self_Attention，就会产生不同的 z。\n",
    "\n",
    "![](https://luweikxy.github.io/machine-learning-notes/natural-language-processing/self-attention-and-transformer/attention-is-all-you-need/pic/multi-headed-attention-4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads   #本来QKV维度：[batch size，seq length,embed dim],拆分后就是[batch size,seq length, h, embed dim/h]\n",
    "                                            # depth 就是用维度除头的个数，也就是 embed dim / h\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)  #初始化qkv矩阵\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size): \n",
    "    \"\"\"拆分embedding dimension维度到 (num_heads, depth)，\n",
    "    这里的 depth=embed dim/h\n",
    "    转置结果使得形状为 (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)   之前没有分头的时候是：[batch size,seq length,embed dim]\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)  转置操作\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model) 级联操作\n",
    "\n",
    "    output = self.dense(concat_attention)  # z  = z * w0  (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add&Norm\n",
    "TODO 需要进一步了解\n",
    "\n",
    "### 残差连接\n",
    "\n",
    "**归纳：模型太深，需要避免梯度消失**  \n",
    "我们在上一步得到了经过注意力矩阵加权之后的 $V$, 也就是 $Attention(Q, \\ K, \\ V)$, 我们对它进行一下转置, 使其和 $X_{embedding}$ 的维度一致, 也就是 $[batch \\ size, \\ sequence \\ length, \\ embedding \\ dimension]$, 然后把他们加起来做残差连接, 直接进行元素相加, 因为他们的维度一致:  \n",
    "$$X_{embedding} + Attention(Q, \\ K, \\ V)$$  \n",
    "在之后的运算里, 每经过一个模块的运算, 都要把运算之前的值和运算之后的值相加, 从而得到残差连接, 训练的时候可以使梯度直接走捷径反传到最初始层:  \n",
    "$$X + SubLayer(X) $$\n",
    "\n",
    "### LayerNorm\n",
    "\n",
    "**归纳：加速收敛**  \n",
    "        $Layer Normalization$ 的作用是把神经网络中隐藏层归一为标准正态分布,  以起到加快训练速度, 加速收敛的作用:  \n",
    "$$ \\mu_i=\\frac {1} {m}\\sum^{m} _ { i=1 } x _ {ij} $$  \n",
    "上式中以矩阵的行 $(row)$ 为单位求均值;  \n",
    "$$\\sigma^{2} _ { j } =\\frac { 1 } { m } \\sum^ { m } _ { i=1 } (x _ { ij } -\\mu_ { j } )^ { 2 } $$  \n",
    "上式中以矩阵的行 $(row)$ 为单位求方差;  \n",
    "$$LayerNorm(x)=\\alpha \\odot \\frac{x_{ij}-\\mu_{i}}  \n",
    "{\\sqrt{\\sigma^{2}_{i}+\\epsilon}} + \\beta \\tag{eq.6}$$  \n",
    "然后用**每一行**的**每一个元素**减去**这行的均值**, 再除以**这行的标准差**, 从而得到归一化后的数值, $\\epsilon$ 是为了防止除 $0$;  \n",
    "之后引入两个可训练参数 $\\alpha, \\ \\beta$ 来弥补归一化的过程中损失掉的信息, 注意 $\\odot$ 表示元素相乘而不是点积, 我们一般初始化 $\\alpha$ 为全 $1$, 而 $\\beta$ 为全 $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise feed-forward networks（点式前馈网络）\n",
    "\n",
    "这层主要是提供非线性变换，是一个全连接层，在编码器和解码器的最后都要使用。`dff`设置了内部全连接层数\n",
    "\n",
    "\n",
    "对此过程中的维度变化进行分析：  \n",
    "![](https://img-blog.csdnimg.cn/20200414165625932.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzU4OTE0,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 编码与解码 \n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20200411142658336.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center%20=500x500)\n",
    "\n",
    "还是这个图最直观，注意观察编码器和解码器的差异，最下面的一块其实差不多，只是解码器加了一个 Mask，这个 mask 当然是 Lookahead mask，因为翻译任务里面，我们是在解码器中输入一个词，解码器拿着编码器最终隐藏层输出的向量来预测下一个词，所以需要去遮盖后面的词。\n",
    "\n",
    "\n",
    "### encoder \n",
    "\n",
    "1). 字向量与位置编码:  \n",
    "$$X = EmbeddingLookup(X) + PositionalEncoding $$  \n",
    "$$X \\in \\mathbb{R}^{batch \\ size \\ * \\ seq. \\ len. \\ * \\ embed. \\ dim.} $$  \n",
    "2). 自注意力机制:  \n",
    "$$Q = Linear(X) = XW_{Q}$$  \n",
    "$$K = Linear(X) = XW_{K} $$  \n",
    "$$V = Linear(X) = XW_{V}$$  \n",
    "$$X_{attention} = SelfAttention(Q, \\ K, \\ V) $$  \n",
    "3). 残差连接与 $Layer \\ Normalization$  \n",
    "$$X_{attention} = X + X_{attention} $$  \n",
    "$$X_{attention} = LayerNorm(X_{attention}) $$  \n",
    "4). $FeedForward$, 其实就是两层线性映射并用激活函数激活, 比如说 $ReLU$:  \n",
    "$$X_{hidden} = Activate(Linear(Linear(X_{attention})))$$  \n",
    "5). 重复 3).:  \n",
    "$$X_{hidden} = X_{attention} + X_{hidden}$$  \n",
    "$$X_{hidden} = LayerNorm(X_{hidden})$$  \n",
    "$$X_{hidden} \\in \\mathbb{R}^{batch \\ size \\ * \\ seq. \\ len. \\ * \\ embed. \\ dim.} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个编码器层包括以下子层：\n",
    "\n",
    "多头注意力（包括填充遮挡）；\n",
    "点式前馈网络。\n",
    "其中的多头注意力其实是输入语句的自注意力，编码器层的输出将会被输入解码器层。\n",
    "\n",
    "每个子层在其周围有一个残差连接，然后进行层归一化。残差连接有助于避免深度网络中的梯度消失问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder\n",
    " **解码器预测过程：** 第 1 时刻——输入'I，解码器拿着编码器输出的 embedding 向量去预测'am'。 第 2 时刻——输入'am'，解码器拿着 embedding 向量 + 'I' 去预测'a' 第 3 时刻——输入'a', 编码器拿着 embedding 向量 + 'I' + 'a' 去预测'student'... [![](https://img-blog.csdnimg.cn/20200411171038906.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center%20=600x300)](https://img-blog.csdnimg.cn/20200411171038906.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70#pic_center%20=600x300)\n",
    "\n",
    "\n",
    "\n",
    " 每个解码器层包括以下子层：\n",
    "\n",
    "*   第一个多头注意力（包括前瞻遮挡和填充遮挡）；\n",
    "*   第二个多头注意力（包括填充遮挡）；\n",
    "*   点式前馈网络。\n",
    "\n",
    "其中，第一个多头注意力其实是目标语句的自注意力，它的 V，K 和 Q 都是来源于目标语句；而第二个多头注意力的 V 和 K 接收编码器输出（即输入语句的自注意力）作为输入。Q 接收第一个多头注意力的输出作为输入。\n",
    "\n",
    "每个子层在其周围有一个残差连接，然后进行层归一化。\n",
    "\n",
    "当 Q 接收到解码器的第一个自注意力块的输出，并且 K 接收到编码器的输出时，注意力权重表示根据编码器的输出赋予解码器输入不同的重要性。换一种说法，解码器通过查看编码器输出和对其自身输出的自注意力来预测下一个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)  \n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    \n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  \n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    # out1 作为 q  输入， enc_output 为 encoder 的输出\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  \n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  \n",
    "    \n",
    "    ffn_output = self.ffn(out2)  \n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  \n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "代码中的`self.mha1`和`self.mha2`就是图中解码器层定义的两个 Multi-Head Attention，这里上面的 Multi-Head Attention 也就是`self.mha2`，它是只做了 padding mask 的，这个和编码器的一致，但是下面的这个 Multi-Head Attention（`self.mha1`）就不一样了，它的 mask 自然是 Lookahead mask，用于遮盖后面的词，现在基本上前后就可以串起来了！  \n",
    "注意看两个的输入：\n",
    "\n",
    "\n",
    "```\n",
    "self.mha1(x, x, x, look_ahead_mask)  \n",
    "self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "```\n",
    "\n",
    "对于下面的 Multi-Head Attention`self.mha1`，它和编码器层那里的代码一致，都是接收三个相同的 x（也就是 q、k、v）  \n",
    "但是对于上面的 Multi-Head Attention`self.mha2`，它的输入是不同的，它是用的编码器的输出和解码器下面的 Multi-Head Attention`self.mha1`的输出`out1`来共同输出`out3`，之前不理解为什么编码器那里要写三个 x，写一个不也可以吗？反正都是一样，现在明白了，是为了和解码器这里的输入做到格式一致！！！ \n",
    "\n",
    "\n",
    "对编码器层和解码器层中的维度变化进行分析：  \n",
    "![](https://img-blog.csdnimg.cn/20200414165706155.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzU4OTE0,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器和解码器构造\n",
    "\n",
    "编码器和解码器无非就是做了 N 个编码器层和解码器层，然后这里的`training`代表的是否训练，因为训练的时候和预测的时候不一样。 \n",
    "![](https://img-blog.csdnimg.cn/20200411173424382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BvcG9mems=,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "编码器包括：\n",
    "\n",
    "*   输入嵌入；\n",
    "*   位置编码；\n",
    "*   N 个编码器层。\n",
    "\n",
    "将输入经过词嵌入层后，再把该嵌入与位置编码相加。该加法操作的输出是编码器层的输入。编码器的输出是解码器的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # 将嵌入和位置编码相加。\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        # https://stackoverflow.com/questions/56930821/why-does-embedding-vector-multiplied-by-a-constant-in-transformer-model\n",
    "        # 这里的乘法增大原始embedding，一个作用是为了令 position embedding 的引入不会太弱化原始 embedding 的含义\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask) # 上层输出是下层的输入\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对编码器和解码器中的维度变化进行分析：  \n",
    "![](https://img-blog.csdnimg.cn/20200414165731548.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzU4OTE0,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码器包括：\n",
    "\n",
    "*   输出embedding；\n",
    "*   位置编码；\n",
    "*   N 个解码器层。\n",
    "\n",
    "将目标语句经过词嵌入层后，再把该嵌入与位置编码相加。该加法操作的结果是解码器层的输入。解码器的输出是最后的线性层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input,\n",
    "                              enc_output=sample_encoder_output,\n",
    "                              training=False,\n",
    "                              look_ahead_mask=None,\n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建 transformer\n",
    "\n",
    "**Transformer** 包括编码器，解码器和最后的线性层。编码器的输出是解码器的输入，解码器的输出是线性层的输入，返回线性层的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                             input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    # Keras models prefer if you pass all your inputs in the first argument\n",
    "    inp, tar = inputs\n",
    "\n",
    "    enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "    return final_output, attention_weights\n",
    "\n",
    "  def create_masks(self, inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 36, 8000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
    "    input_vocab_size=8500, target_vocab_size=8000,\n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对此过程中的维度变化进行分析：  \n",
    "![](https://img-blog.csdnimg.cn/20200414165752622.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzU4OTE0,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置超参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   num_layers：编码器层数和解码器层数；\n",
    "*   d_model：词嵌入维度；\n",
    "*   dff：点式前馈网络中第一个全连接层的神经元个数；\n",
    "*   num_heads：多头注意力中的头数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里我们使用 Adam 优化器，但我们规定学习率为：\n",
    "\n",
    "lrate $=d_{\\text {model }}^{-0.5} * \\min \\left(\\right.$ step_num $^{-0.5}$, step_num $\\cdot$ warmup_steps $\\left.^{-1.5}\\right)$\n",
    "\n",
    "先让学习率线性增长到某个最大的值，然后再按指数的方式衰减,这个方式对于训练Transformer来说，是一个比较重要的策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失和度量\n",
    "\n",
    "在这里我们使用了遮挡（mask），这样就可以把我们填充的位置的损失置为 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    \n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造模型\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建检查点\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标语句被分成了 **tar_inp** 和 **tar_real**。**tar_inp** 作为输入被传递到解码器。**tar_real** 是位移了 1 的同一个输入：在 **tar_inp** 中的每个位置，**tar_real** 包含了应该被预测到的下一个标记。\n",
    "\n",
    "例如，目标语句为 “SOS A lion in the jungle is sleeping EOS”，那么：  \n",
    "tar_inp = “SOS A lion in the jungle is sleeping”  \n",
    "tar_real = “A lion in the jungle is sleeping EOS”\n",
    "\n",
    "Transformer 是一个自回归模型：它一次作一个部分的预测，然后使用到目前为止的自身的输出来决定下一步要做什么。\n",
    "\n",
    "在训练过程中，我们使用了 teacher-forcing 的方法(like in the [text generation tutorial](https://www.tensorflow.org/text/tutorials/text_generation))。无论模型在当前时间步骤下预测出什么，teacher-forcing 方法都会将真实的输出传递到下一个时间步骤上。\n",
    "\n",
    "当 Transformer 预测每个词时，自注意力（self-attention）功能使它能够查看输入序列中前面的单词，从而更好地预测下一个单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "# 该 @tf.function 将追踪-编译 train_step 到 TF 图中，以便更快地\n",
    "# 执行。该函数专用于参数张量的精确形状。为了避免由于可变序列长度或可变\n",
    "# 批次大小（最后一批次较小）导致的再追踪，使用 input_signature 指定\n",
    "# 更多的通用形状。\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer([inp, tar_inp],\n",
    "                                 training = True)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x0000025BCD6C6510> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x0000025BCD6C6510> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function train_step at 0x0000025BCD6C6510> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Transformer.call of <__main__.Transformer object at 0x0000025BCD1BFF60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Transformer.call of <__main__.Transformer object at 0x0000025BCD1BFF60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Transformer.call of <__main__.Transformer object at 0x0000025BCD1BFF60>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Encoder.call of <__main__.Encoder object at 0x0000025BCD1DEA20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Encoder.call of <__main__.Encoder object at 0x0000025BCD1DEA20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Encoder.call of <__main__.Encoder object at 0x0000025BCD1DEA20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method EncoderLayer.call of <__main__.EncoderLayer object at 0x0000025BCD63DA58>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method EncoderLayer.call of <__main__.EncoderLayer object at 0x0000025BCD63DA58>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method EncoderLayer.call of <__main__.EncoderLayer object at 0x0000025BCD63DA58>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x0000025BCD63D940>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x0000025BCD63D940>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x0000025BCD63D940>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Decoder.call of <__main__.Decoder object at 0x0000025BCD6AE2B0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Decoder.call of <__main__.Decoder object at 0x0000025BCD6AE2B0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Decoder.call of <__main__.Decoder object at 0x0000025BCD6AE2B0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method DecoderLayer.call of <__main__.DecoderLayer object at 0x0000025BCD6AE5C0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method DecoderLayer.call of <__main__.DecoderLayer object at 0x0000025BCD6AE5C0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method DecoderLayer.call of <__main__.DecoderLayer object at 0x0000025BCD6AE5C0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1 Batch 0 Loss 8.8673 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.7974 Accuracy 0.0076\n",
      "Epoch 1 Batch 100 Loss 8.6900 Accuracy 0.0299\n",
      "Epoch 1 Batch 150 Loss 8.5712 Accuracy 0.0403\n",
      "Epoch 1 Batch 200 Loss 8.4279 Accuracy 0.0454\n",
      "Epoch 1 Batch 250 Loss 8.2568 Accuracy 0.0495\n",
      "Epoch 1 Batch 300 Loss 8.0665 Accuracy 0.0543\n",
      "Epoch 1 Batch 350 Loss 7.8688 Accuracy 0.0593\n",
      "Epoch 1 Batch 400 Loss 7.6802 Accuracy 0.0651\n",
      "Epoch 1 Batch 450 Loss 7.5188 Accuracy 0.0719\n",
      "Epoch 1 Batch 500 Loss 7.3725 Accuracy 0.0788\n",
      "Epoch 1 Batch 550 Loss 7.2394 Accuracy 0.0858\n",
      "Epoch 1 Batch 600 Loss 7.1162 Accuracy 0.0931\n",
      "Epoch 1 Batch 650 Loss 7.0001 Accuracy 0.1002\n",
      "Epoch 1 Batch 700 Loss 6.8928 Accuracy 0.1066\n",
      "Epoch 1 Batch 750 Loss 6.7928 Accuracy 0.1127\n",
      "Epoch 1 Batch 800 Loss 6.6995 Accuracy 0.1184\n",
      "Epoch 1 Loss 6.6828 Accuracy 0.1194\n",
      "Time taken for 1 epoch: 1320.20 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 5.3584 Accuracy 0.2023\n",
      "Epoch 2 Batch 50 Loss 5.2343 Accuracy 0.2081\n",
      "Epoch 2 Batch 100 Loss 5.2143 Accuracy 0.2118\n",
      "Epoch 2 Batch 150 Loss 5.1857 Accuracy 0.2152\n",
      "Epoch 2 Batch 200 Loss 5.1604 Accuracy 0.2192\n",
      "Epoch 2 Batch 250 Loss 5.1306 Accuracy 0.2224\n",
      "Epoch 2 Batch 300 Loss 5.1049 Accuracy 0.2249\n",
      "Epoch 2 Batch 350 Loss 5.0766 Accuracy 0.2278\n",
      "Epoch 2 Batch 400 Loss 5.0511 Accuracy 0.2301\n",
      "Epoch 2 Batch 450 Loss 5.0306 Accuracy 0.2320\n",
      "Epoch 2 Batch 500 Loss 5.0064 Accuracy 0.2344\n",
      "Epoch 2 Batch 550 Loss 4.9881 Accuracy 0.2361\n",
      "Epoch 2 Batch 600 Loss 4.9688 Accuracy 0.2380\n",
      "Epoch 2 Batch 650 Loss 4.9496 Accuracy 0.2398\n",
      "Epoch 2 Batch 700 Loss 4.9333 Accuracy 0.2413\n",
      "Epoch 2 Batch 750 Loss 4.9149 Accuracy 0.2429\n",
      "Epoch 2 Batch 800 Loss 4.8992 Accuracy 0.2442\n",
      "Epoch 2 Loss 4.8963 Accuracy 0.2445\n",
      "Time taken for 1 epoch: 1244.70 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 4.6171 Accuracy 0.2752\n",
      "Epoch 3 Batch 50 Loss 4.6064 Accuracy 0.2693\n",
      "Epoch 3 Batch 100 Loss 4.5825 Accuracy 0.2724\n",
      "Epoch 3 Batch 150 Loss 4.5694 Accuracy 0.2734\n",
      "Epoch 3 Batch 200 Loss 4.5514 Accuracy 0.2754\n",
      "Epoch 3 Batch 250 Loss 4.5422 Accuracy 0.2764\n",
      "Epoch 3 Batch 300 Loss 4.5340 Accuracy 0.2769\n",
      "Epoch 3 Batch 350 Loss 4.5273 Accuracy 0.2778\n",
      "Epoch 3 Batch 400 Loss 4.5138 Accuracy 0.2792\n",
      "Epoch 3 Batch 450 Loss 4.4982 Accuracy 0.2809\n",
      "Epoch 3 Batch 500 Loss 4.4853 Accuracy 0.2821\n",
      "Epoch 3 Batch 550 Loss 4.4719 Accuracy 0.2837\n",
      "Epoch 3 Batch 600 Loss 4.4567 Accuracy 0.2854\n",
      "Epoch 3 Batch 650 Loss 4.4445 Accuracy 0.2867\n",
      "Epoch 3 Batch 700 Loss 4.4307 Accuracy 0.2883\n",
      "Epoch 3 Batch 750 Loss 4.4155 Accuracy 0.2902\n",
      "Epoch 3 Batch 800 Loss 4.3987 Accuracy 0.2925\n",
      "Epoch 3 Loss 4.3955 Accuracy 0.2928\n",
      "Time taken for 1 epoch: 1191.21 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 4.1471 Accuracy 0.3115\n",
      "Epoch 4 Batch 50 Loss 4.0530 Accuracy 0.3281\n",
      "Epoch 4 Batch 100 Loss 4.0419 Accuracy 0.3312\n",
      "Epoch 4 Batch 150 Loss 4.0250 Accuracy 0.3347\n",
      "Epoch 4 Batch 200 Loss 4.0119 Accuracy 0.3361\n",
      "Epoch 4 Batch 250 Loss 3.9992 Accuracy 0.3381\n",
      "Epoch 4 Batch 300 Loss 3.9908 Accuracy 0.3393\n",
      "Epoch 4 Batch 350 Loss 3.9760 Accuracy 0.3413\n",
      "Epoch 4 Batch 400 Loss 3.9602 Accuracy 0.3434\n",
      "Epoch 4 Batch 450 Loss 3.9434 Accuracy 0.3454\n",
      "Epoch 4 Batch 500 Loss 3.9291 Accuracy 0.3472\n",
      "Epoch 4 Batch 550 Loss 3.9160 Accuracy 0.3489\n",
      "Epoch 4 Batch 600 Loss 3.9029 Accuracy 0.3508\n",
      "Epoch 4 Batch 650 Loss 3.8891 Accuracy 0.3527\n",
      "Epoch 4 Batch 700 Loss 3.8750 Accuracy 0.3547\n",
      "Epoch 4 Batch 750 Loss 3.8616 Accuracy 0.3565\n",
      "Epoch 4 Batch 800 Loss 3.8473 Accuracy 0.3585\n",
      "Epoch 4 Loss 3.8449 Accuracy 0.3588\n",
      "Time taken for 1 epoch: 1188.35 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.6303 Accuracy 0.3817\n",
      "Epoch 5 Batch 50 Loss 3.5363 Accuracy 0.3940\n",
      "Epoch 5 Batch 100 Loss 3.5132 Accuracy 0.3970\n",
      "Epoch 5 Batch 150 Loss 3.5001 Accuracy 0.3996\n",
      "Epoch 5 Batch 200 Loss 3.4897 Accuracy 0.4016\n",
      "Epoch 5 Batch 250 Loss 3.4784 Accuracy 0.4031\n",
      "Epoch 5 Batch 300 Loss 3.4770 Accuracy 0.4035\n",
      "Epoch 5 Batch 350 Loss 3.4692 Accuracy 0.4044\n",
      "Epoch 5 Batch 400 Loss 3.4649 Accuracy 0.4049\n",
      "Epoch 5 Batch 450 Loss 3.4532 Accuracy 0.4065\n",
      "Epoch 5 Batch 500 Loss 3.4398 Accuracy 0.4083\n",
      "Epoch 5 Batch 550 Loss 3.4344 Accuracy 0.4090\n",
      "Epoch 5 Batch 600 Loss 3.4234 Accuracy 0.4105\n",
      "Epoch 5 Batch 650 Loss 3.4128 Accuracy 0.4120\n",
      "Epoch 5 Batch 700 Loss 3.4029 Accuracy 0.4135\n",
      "Epoch 5 Batch 750 Loss 3.3941 Accuracy 0.4148\n",
      "Epoch 5 Batch 800 Loss 3.3850 Accuracy 0.4161\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train\\ckpt-1\n",
      "Epoch 5 Loss 3.3834 Accuracy 0.4163\n",
      "Time taken for 1 epoch: 1197.92 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 3.1242 Accuracy 0.4410\n",
      "Epoch 6 Batch 50 Loss 3.1003 Accuracy 0.4477\n",
      "Epoch 6 Batch 100 Loss 3.1157 Accuracy 0.4465\n",
      "Epoch 6 Batch 150 Loss 3.1054 Accuracy 0.4488\n",
      "Epoch 6 Batch 200 Loss 3.0925 Accuracy 0.4499\n",
      "Epoch 6 Batch 250 Loss 3.0894 Accuracy 0.4506\n",
      "Epoch 6 Batch 300 Loss 3.0822 Accuracy 0.4513\n",
      "Epoch 6 Batch 350 Loss 3.0705 Accuracy 0.4529\n",
      "Epoch 6 Batch 400 Loss 3.0558 Accuracy 0.4552\n",
      "Epoch 6 Batch 450 Loss 3.0478 Accuracy 0.4560\n",
      "Epoch 6 Batch 500 Loss 3.0365 Accuracy 0.4575\n",
      "Epoch 6 Batch 550 Loss 3.0258 Accuracy 0.4589\n",
      "Epoch 6 Batch 600 Loss 3.0186 Accuracy 0.4600\n",
      "Epoch 6 Batch 650 Loss 3.0102 Accuracy 0.4612\n",
      "Epoch 6 Batch 700 Loss 3.0031 Accuracy 0.4622\n",
      "Epoch 6 Batch 750 Loss 2.9949 Accuracy 0.4635\n",
      "Epoch 6 Batch 800 Loss 2.9885 Accuracy 0.4645\n",
      "Epoch 6 Loss 2.9866 Accuracy 0.4648\n",
      "Time taken for 1 epoch: 1208.86 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.5897 Accuracy 0.5073\n",
      "Epoch 7 Batch 50 Loss 2.7123 Accuracy 0.4996\n",
      "Epoch 7 Batch 100 Loss 2.7075 Accuracy 0.4993\n",
      "Epoch 7 Batch 150 Loss 2.7017 Accuracy 0.5002\n",
      "Epoch 7 Batch 200 Loss 2.6977 Accuracy 0.5007\n",
      "Epoch 7 Batch 250 Loss 2.6885 Accuracy 0.5024\n",
      "Epoch 7 Batch 300 Loss 2.6794 Accuracy 0.5037\n",
      "Epoch 7 Batch 350 Loss 2.6802 Accuracy 0.5035\n",
      "Epoch 7 Batch 400 Loss 2.6786 Accuracy 0.5036\n",
      "Epoch 7 Batch 450 Loss 2.6742 Accuracy 0.5042\n",
      "Epoch 7 Batch 500 Loss 2.6685 Accuracy 0.5049\n",
      "Epoch 7 Batch 550 Loss 2.6588 Accuracy 0.5062\n",
      "Epoch 7 Batch 600 Loss 2.6547 Accuracy 0.5068\n",
      "Epoch 7 Batch 650 Loss 2.6520 Accuracy 0.5073\n",
      "Epoch 7 Batch 700 Loss 2.6491 Accuracy 0.5077\n",
      "Epoch 7 Batch 750 Loss 2.6466 Accuracy 0.5081\n",
      "Epoch 7 Batch 800 Loss 2.6445 Accuracy 0.5083\n",
      "Epoch 7 Loss 2.6441 Accuracy 0.5083\n",
      "Time taken for 1 epoch: 1190.88 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.6256 Accuracy 0.5109\n",
      "Epoch 8 Batch 50 Loss 2.4232 Accuracy 0.5371\n",
      "Epoch 8 Batch 100 Loss 2.4224 Accuracy 0.5371\n",
      "Epoch 8 Batch 150 Loss 2.4295 Accuracy 0.5354\n",
      "Epoch 8 Batch 200 Loss 2.4183 Accuracy 0.5370\n",
      "Epoch 8 Batch 250 Loss 2.4196 Accuracy 0.5374\n"
     ]
    }
   ],
   "source": [
    "# 葡萄牙语输入，英语输出\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "\n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_batches):\n",
    "    train_step(inp, tar)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "\n",
    "\n",
    "\n",
    "- 使用葡萄牙语标记器 (tokenizers.pt) 对输入句子进行编码。 这是编码器输入。\n",
    "- 解码器输入初始化为 [START] 标记。\n",
    "- 计算padding mask和 look ahead mask。\n",
    "- 然后解码器通过查看编码器输出和它自己的输出（self-attention）来输出预测。\n",
    "- 将预测的结果连接到解码器输入，并将其传递给解码器。\n",
    "- 在这种方法中，解码器根据它预测的之前一个词来预测下一个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  def __init__(self, tokenizers, transformer):\n",
    "    self.tokenizers = tokenizers\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, sentence, max_length=20):\n",
    "    # input sentence is portuguese, hence adding the start and end token\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence\n",
    "\n",
    "    # as the target is english, the first token to the transformer should be the\n",
    "    # english start token.\n",
    "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "    start = start_end[0][tf.newaxis]\n",
    "    end = start_end[1][tf.newaxis]\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a python list) so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())\n",
    "      predictions, _ = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # select the last token from the seq_len dimension\n",
    "      predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # concatentate the predicted_id to the output which is given to the decoder\n",
    "      # as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # output.shape (1, tokens)\n",
    "    text = tokenizers.en.detokenize(output)[0]  # shape: ()\n",
    "\n",
    "    tokens = tokenizers.en.lookup(output)[0]\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop. So recalculate them outside\n",
    "    # the loop.\n",
    "    _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "\n",
    "    return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, transformer)\n",
    "def print_translation(sentence, tokens, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"este é um problema que temos que resolver.\"\n",
    "ground_truth = \"this is a problem we have to solve .\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\"\n",
    "ground_truth = \"so i \\'ll just share with you some stories very quickly of some magical things that have happened .\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"os meus vizinhos ouviram sobre esta ideia.\"\n",
    "ground_truth = \"and my neighboring homes heard about this idea .\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力权重图\n",
    "\n",
    "Translator 类返回的 attention 字典可以用来可视化模型的内部工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"este é o primeiro livro que eu fiz.\"\n",
    "ground_truth = \"this is the first book i've ever done.\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
    "  # The plot is of the attention when a token was generated.\n",
    "  # The model didn't generate `<START>` in the output. Skip it.\n",
    "  translated_tokens = translated_tokens[1:]\n",
    "\n",
    "  ax = plt.gca()\n",
    "  ax.matshow(attention)\n",
    "  ax.set_xticks(range(len(in_tokens)))\n",
    "  ax.set_yticks(range(len(translated_tokens)))\n",
    "\n",
    "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
    "  ax.set_xticklabels(\n",
    "      labels, rotation=90)\n",
    "\n",
    "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
    "  ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = 0\n",
    "# shape: (batch=1, num_heads, seq_len_q, seq_len_k)\n",
    "attention_heads = tf.squeeze(\n",
    "  attention_weights['decoder_layer4_block2'], 0)\n",
    "attention = attention_heads[head]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_tokens = tf.convert_to_tensor([sentence])\n",
    "in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
    "in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
    "in_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_head(in_tokens, translated_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
    "  in_tokens = tf.convert_to_tensor([sentence])\n",
    "  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
    "  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
    "  in_tokens\n",
    "\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "  for h, head in enumerate(attention_heads):\n",
    "    ax = fig.add_subplot(2, 4, h+1)\n",
    "\n",
    "    plot_attention_head(in_tokens, translated_tokens, head)\n",
    "\n",
    "    ax.set_xlabel(f'Head {h+1}')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_weights(sentence, translated_tokens,\n",
    "                       attention_weights['decoder_layer4_block2'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Eu li sobre triceratops na enciclopédia.\"\n",
    "ground_truth = \"I read about triceratops in the encyclopedia.\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)\n",
    "\n",
    "plot_attention_weights(sentence, translated_tokens,\n",
    "                       attention_weights['decoder_layer4_block2'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型导出\n",
    "\n",
    "模型预测结果合适，就可以导出模型了 `tf.saved_model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仅输出了预测结果句子\n",
    "class ExportTranslator(tf.Module):\n",
    "  def __init__(self, translator):\n",
    "    self.translator = translator\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def __call__(self, sentence):\n",
    "    (result, \n",
    "     tokens,\n",
    "     attention_weights) = self.translator(sentence, max_length=100)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = ExportTranslator(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator(\"este é o primeiro livro que eu fiz.\").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(translator, export_dir='translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.saved_model.load('translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded(\"este é o primeiro livro que eu fiz.\").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78deabf0d9e733d3d2ccaeb80c3b54160b2da219477fa73c78a226312c9bdd1d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
