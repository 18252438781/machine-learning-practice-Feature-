# 分类问题评估指标



本文主要介绍二分类场景下的评估指标，包括准确率、召回率、F1 ，P-R 曲线，ROC 曲线 和 AUC。



## 一、混淆矩阵、准确率、精确率与召回率



### 1.1 混淆矩阵（Confusion Matrix）

![image-20200811201754476](https://gitee.com/skyexu/images/raw/master/img/confusion_matrix.png)

- TP（true positive）： 真正例，即把正例正确预测为正例。
- FP（false positive）： 假正例，即把负例错误预测为正例。
- FN（false negative）： 假负例，即把正例错误预测为负例。
- TN（true negative）： 真负例，即把负例正确预测为负例。



### 1.2 准确率（accuracy）

评估分类器的整体准确率，即预测正确的结果占总样本的比例，其中预测是否正确包括将正例预测为正例（TP）与将负例正确预测为负例（TN）。
$$
accuracy=\frac{TP+TN}{TP+TN+FP+FN}
$$


### 1.3 精确率（查准率）（Precision）



“所有挑出来的瓜，有多少是好瓜”，“推荐结果中有多少是用户真正感兴趣的”，在这类场景中，为了评估预测为正例的结果中，有多少真正是正例的评估指标为精确率，或者用**查准率**更好理解，计算方式如下：
$$
Precision=\frac{TP}{TP+FP}
$$

### 1.4 召回率（查全率）（Recall）

“所有好瓜有多少被挑出来了”，“用户感兴趣的内容有多少被推荐了”，在这类场景中，为了评估所有的正样本有多少被预测为正样本的评估指标为召回率，又称为**查全率**，计算方式如下：
$$
Recall=\frac{TP}{TP+FN}
$$

### 1.5 P-R 曲线

查全率与查准率是一对矛盾的度量，一般情况下，查准率高的时候，查全率往往偏低；查全率高的时候，查准率往往偏低。一般来说，我们以0.5作为分类的阈值，大于0.5认为是正例，小于0.5认为是负例。不断更改这个分类阈值可以画出查全率与查准率的关系，即 P-R 曲线。

![1](https://gitee.com/skyexu/images/raw/master/img/20160721111312507.jpg)

上图展示了查全率和查准率随阈值变化的关系，曲线下的面积一定程度上代表了学习器在查准率和查全率上取得“双高”的比例，可以作为评估两个机器学习模型的性能优劣的标准。如果一个学习器的PR曲线被另一个包住，则说明后者优于前者。PR曲线有以下特点：

1. 曲线是震荡的，不平滑的。假设分类阈值变低，那么正例被判断为正例的变化，同样负例被判断为正例的也可能变多，查全率变高，查准率可能变高也会变低，整体呈下降趋势。
2. 曲线不会经过（1，0）点，查准率只可能趋向于0，比如负样本很多，正样本很少的情况下，正样本都被召回。



### 1.6 F1

F1 指标综合考虑查准率与查全率，是基于两者的调和平均定义的。
$$
F1=\frac{1}{2}\times(\frac{1}{P}+\frac{1}{R})=\frac{2 \times P \times R}{P+R}
$$

## 二、ROC、AUC

### 1.1 ROC

在实际场景中我们经常根据模型输出的概率值对结果进行排序，我们需要一个指标能够衡量学习器排序能力的好坏（正样本排在负样本前面的情况），这时候 ROC/AUC 就派上用场了。

ROC（Receiver Operating Characteristic）曲线，又称接受者操作特征曲线。该曲线最早应用于敌机检测的雷达信号分析技术，二十世纪六七十年代开始被用于一些心理学、医学检测应用中，后被引入机器学习领域。这里引入两个指标，分别为真正例率（True Positive Rate，TPR），假正例率（False Positive Rate, FPR）。
$$
TPR=\frac{TP}{TP+FN}
$$

$$
FPR=\frac{FP}{TN+FP}
$$

- TPR 的意义是所有真实类别为1的样本中，预测类别为1的比例，即正例覆盖的情况。
- FPR 的意义是所有真实类别为0的样本中，预测类别为1的比例，即正例虚报的情况。

显然，TPR 越高越好，FPR 越低越好。

ROC 曲线的横坐标是 FPR，纵坐标为 TPR。每次选取不同的分类阈值，得到多组 TPR 和 FPR，即 ROC 曲线上的点，即可画出ROC曲线，真实情况下曲线更接近与下图 （b），非光滑。

![](https://gitee.com/Skyexu/images/raw/master/img/image-20200816001540785.png)

### 1.2 AUC

AUC （Area Under Curve）为ROC 曲线下的面积。当 TPR 和 FPR 都相等，即满足 y=x 曲线，AUC = 0.5 说明学习器对正例和负例完全没有区分能力。我们希望 TPR 大于 FPR，所以曲线一般往左上角凸，AUC 越高越好，最大值为 1。

AUC 另一个解释：AUC值是一个概率值，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。



**AUC 的计算方式：**

**方式一：**

直接计算 ROC 曲线下面积，缺点是计算繁琐。



**方式二：**

计算正例大于负例的概率，即统计所有 M×N (M为正类样本的数目，N为负类样本的数目)个正负样本对中，有多少组正样本的预测值大于负样本的预测值。
$$
\frac{\sum I\left(P_{\text {正样本 }}, P_{\text {负样本 }}\right)}{M^{*} N}，
$$

$$
其中，I\left(P_{\text {正样本 }}, P_{\text {负样本 }}\right)=\left\{\begin{array}{l}1, P_{\text {正样本 }}>P_{\text {正样本 }} \\ 0.5, P_{\text {正祥本 }}=P_{\text {负祥本 }} \\ 0, P_{\text {正样本 }}<P_{\text {负样本 }}\end{array}\right.
$$



比如以下样本

|      | 预测值(score) | 真实标签 |
| ---- | ------------- | -------- |
| A    | 0.2           | 0        |
| B    | 0.5           | 0        |
| C    | 0.3           | 1        |
| D    | 0.8           | 1        |

其中正例有2个，负例有2个，可以构成 2 * 2 = 4 个正负样本对（[0.3,0.2],[0.8,0.2],[0.3,0.5],[0.8,0.5]），其中正例预测值大于负例的有 3 对 （[0.3,0.2],[0.8,0.2],[0.8,0.5]），因此 AUC 为 (1+1+1)/4 = 0.75。 如果上述例子中样本C的预测值为 0.5，即样本2与3预测score一样，则 AUC = (1+1+1+0.5) /4 = 0.875。这种计算方法复杂度较高，为 O(M*N)。



**方式三：**
$$
AUC=\frac{\sum_{{i} \in \text {positiveclass}} \text {rank}_{i}-\frac{M \times(M+1)}{2}}{M \times N}
$$


- ${rank}_{i}$: 对预测 score 从小到大排序，正样本的排序值；
- M: 正样本个数；
- N: 负样本个数；

对所有预测值排序后 ${rank}_{i}$ 可以表示当前位置和之前的位置能够构造出多少个样本对（其中包括自己），再减去正例-正例这种组合的个数 M*(M+1)/2 ，即为正例-负例这种组合的个数，再除以 M * N 即可得到 AUC 值。这种方式计算 AUC 的复杂度为排序复杂度，即$O(n \log (n))$。

以下述例子为例：

|      | 预测值(score) | 真实标签 | rank |
| ---- | ------------- | -------- | ---- |
| A    | 0.2           | 0        | 1    |
| B    | 0.5           | 0        | 3    |
| C    | 0.3           | 1        | 2    |
| D    | 0.8           | 1        | 4    |

正例的排序索引为2和4，M和N都为2，因此 AUC=(2+4-2 * (2+1)/2) / (2 * 2)=0.75。



如果其中有相同的预测值，如下情况

| ID   | 预测值(score) | 真实标签 | rank |
| ---- | ------------- | -------- | ---- |
| A    | 0.3           | 0        | 1    |
| B    | 0.5           | 1        | 2    |
| C    | 0.5           | 1        | 3    |
| D    | 0.5           | 0        | 4    |
| E    | 0.5           | 0        | 5    |
| F    | 0.7           | 1        | 6    |
| G    | 0.8           | 1        | 7    |

则需要对相同预测值的 rank 取平均，生成新的rank值，auc 计算结果如下：
$$
\frac{7+6+\frac{(5+4+3+2)}{4}+\frac{(5+4+3+2)}{4}-\frac{4 *(4+1)}{2}}{4 * 3}=\frac{10}{12}
$$
 

