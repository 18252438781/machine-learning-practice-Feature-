# 分类问题评估指标



本文主要介绍二分类场景下的评估指标，包括准确率、召回率、F1 ，P-R 曲线，ROC 曲线 和 AUC。



## 一、混淆矩阵、准确率、精确率与召回率



### 1.1 混淆矩阵（Confusion Matrix）

![image-20200811201754476](https://gitee.com/skyexu/images/raw/master/img/confusion_matrix.png)

- TP（true positive）： 真正例，即把正例正确预测为正例。
- FP（false positive）： 假正例，即把负例错误预测为正例。
- FN（false negative）： 假负例，即把正例错误预测为负例。
- TN（true negative）： 真负例，即把负例正确预测为负例。

### 

### 1.2 准确率（accuracy）

评估分类器的整体准确率，即预测正确的结果占总样本的比例，其中预测是否正确包括将正例预测为正例（TP）与将负例正确预测为负例（TN）。
$$
accuracy=\frac{TP+TN}{(TP+TN+FP+FN)}
$$


### 1.3 精确率（查准率）（Precision）



“所有挑出来的瓜，有多少是好瓜”，“推荐结果中有多少是用户真正感兴趣的”，在这类场景中，为了评估预测为正例的结果中，有多少真正是正例的评估指标为精确率，或者用**查准率**更好理解，计算方式如下：
$$
Precision=\frac{TP}{(TP+FP)}
$$

### 1.4 召回率（查全率）（Recall）

“所有好瓜有多少被挑出来了”，“用户感兴趣的内容有多少被推荐了”，在这类场景中，为了评估所有的正样本有多少被预测为正样本的评估指标为召回率，又称为**查全率**，计算方式如下：
$$
Recall=\frac{TP}{(TP+FN)}
$$

### 1.5 P-R 曲线

查全率与查准率是一对矛盾的度量，一般情况下，查准率高的时候，查全率往往偏低；查全率高的时候，查准率往往偏低。一般来说，我们以0.5作为分类的阈值，大于0.5认为是正例，小于0.5认为是负例。不断更改这个分类阈值可以画出查全率与查准率的关系，即 P-R 曲线。

![1](https://gitee.com/skyexu/images/raw/master/img/20160721111312507.jpg)

上图展示了查全率和查准率随阈值变化的关系，曲线下的面积一定程度上代表了学习器在查准率和查全率上取得“双高”的比例，可以作为评估两个机器学习模型的性能优劣的标准。如果一个学习器的PR曲线被另一个包住，则说明后者优于前者。PR曲线有以下特点：

1. 曲线是震荡的，不平滑的。假设分类阈值变低，那么正例被判断为正例的变化，同样负例被判断为正例的也可能变多，查全率变高，查准率可能变高也会变低，整体呈下降趋势。
2. 曲线不会经过（1，0）点，查准率只可能趋向于0，比如负样本很多，正样本很少的情况下，正样本都被召回。



### 1.6 F1

F1 指标综合考虑查准率与查全率，是基于两者的调和平均定义的。
$$
F1=\frac{1}{2}\times(\frac{1}{P}+\frac{1}{R})=\frac{2 \times P \times R}{P+R}
$$

## 二、ROC、AUC

在实际场景中我们经常根据模型输出的概率值对结果进行排序，我们需要一个指标能够衡量学习器排序能力的好坏。